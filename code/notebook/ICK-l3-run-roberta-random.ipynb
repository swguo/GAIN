{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persuasive Technical News Detection Task\n",
    "\n",
    "專案描述 \\\n",
    "這個專案採用 PyTorch Lightning 框架，執行 NLP4IF2019 News 分類任務。訓練和驗證過程會上傳至 Weights & Biases (wandb) 平台，並在驗證階段計算混淆矩陣（confusion matrix）和 AUC（Area Under the Curve）分數。\n",
    "\n",
    "NLP4IF2019 News 資料集介紹 \\\n",
    "NLP4IF2019 News 資料集是由NLP4IF2019 share task收集並整理的一個新聞數據集，數據集包括350篇訓練文章、61篇開發文章和86篇測試文章，共有1,2000 Passages。這些文章均來自48家不同的新聞媒體。文章中的片段被標註了18種宣傳技術之一。\n",
    "\n",
    "資料集特點： \\\n",
    "數據量： 12000 條新聞評論 \\\n",
    "標籤：多標籤分類（multi-label classification），即18種宣傳技術 \\\n",
    "語言：英文（English） \\\n",
    "應用場景：文本分類（text classification）\n",
    "\n",
    "github : https://github.com/Tariq60/propaganda-nlp4if2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'PersuTech-adapter-4c-DIL'\n",
    "model_name_or_path = 'cardiffnlp/twitter-roberta-large-2022-154m' #'FacebookAI/roberta-large'\n",
    "task_name = \"roberta-Adapter-CE,len=256\"\n",
    "version = 'ICK-L3'\n",
    "load_checkpoint = 'True'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user_data/envs/adapter/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-2022-154m and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "load dataset: /user_data/workspace/CL-PersuTech-4c-DIL/1_adapter/../data/NLP4IF2019/Encoder-based-hierarchical/train/Increasing(ICK)/Level/dataset\n",
      "[0.536231884057971, 0.14492753623188406, 0.18840579710144928, 0.2028985507246377]\n",
      "[0.536231884057971, 0.605072463768116, 0.5615942028985508, 0.5471014492753623]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-2022-154m and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Conver to Embeddings: 100%|█████████████████████| 69/69 [00:01<00:00, 59.90it/s]\n",
      "The 'config' and 'model_name' arguments are specific to the now unsupported legacy Hub repo and will be removed.Please switch to only providing the HF Model Hub identifier.\n",
      "/user_data/envs/adapter/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:208: Attribute 'adapter_model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['adapter_model'])`.\n",
      "focal loss, alpha\n",
      "[0.536231884057971, 0.605072463768116, 0.5615942028985508, 0.5471014492753623]\n",
      "ContrastiveLearningModel(\n",
      "  (encoder): RobertaAdapterModel(\n",
      "    (roberta): RobertaModel(\n",
      "      (embeddings): RobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 1024)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-23): 24 x RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttentionWithAdapters(\n",
      "                (query): LoRALinearTorch(\n",
      "                  in_features=1024, out_features=1024, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): LoRALinearTorch(\n",
      "                  in_features=1024, out_features=1024, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): LoRALinearTorch(\n",
      "                  in_features=1024, out_features=1024, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningLayer(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): RobertaSelfOutputWithAdapters(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): LoRALinearTorch(\n",
      "                in_features=1024, out_features=4096, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutputWithAdapters(\n",
      "              (dense): LoRALinearTorch(\n",
      "                in_features=4096, out_features=1024, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (task_adapter): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=1024, out_features=64, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=64, out_features=1024, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "            (reft_layer): ReftLayer(\n",
      "              (refts): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): RobertaPooler(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (invertible_adapters): ModuleDict()\n",
      "      (shared_parameters): ModuleDict()\n",
      "      (prefix_tuning): PrefixTuningPool(\n",
      "        (prefix_tunings): ModuleDict()\n",
      "      )\n",
      "      (prompt_tuning): PromptTuningLayer(\n",
      "        (base_model_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
      "        (prompt_tunings): ModuleDict()\n",
      "      )\n",
      "    )\n",
      "    (heads): ModuleDict(\n",
      "      (default): BertStyleMaskedLMHead(\n",
      "        (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (1): Activation_Function_Class(\n",
      "          (f): GELUActivation()\n",
      "        )\n",
      "        (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (3): Linear(in_features=1024, out_features=50265, bias=True)\n",
      "      )\n",
      "      (task_adapter): ClassificationHead(\n",
      "        (0): Dropout(p=0.1, inplace=False)\n",
      "        (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (2): Activation_Function_Class(\n",
      "          (f): Tanh()\n",
      "        )\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=1024, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/user_data/envs/adapter/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:44: Attribute 'adapter_model' removed from hparams because it cannot be pickled. You can suppress this warning by setting `self.save_hyperparameters(ignore=['adapter_model'])`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mswguo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240919_173450-bgzd9i34\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mroberta-Adapter-CE,len=256_ICK-L3_random\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/swguo/PersuTech-adapter-4c-DIL\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/swguo/PersuTech-adapter-4c-DIL/runs/bgzd9i34\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/user_data/envs/adapter/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name    | Type                | Params | Mode\n",
      "-------------------------------------------------------\n",
      "0 | encoder | RobertaAdapterModel | 360 M  | eval\n",
      "-------------------------------------------------------\n",
      "5.3 M     Trainable params\n",
      "355 M     Non-trainable params\n",
      "360 M     Total params\n",
      "1,442.749 Total estimated model params size (MB)\n",
      "/user_data/envs/adapter/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (17) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "Epoch 0:  59%|▌| 10/17 [00:01<00:01,  6.39it/s, v_num=9i34, train_loss_step=1.51\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                  | 1/39 [00:00<00:20,  1.88it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                  | 2/39 [00:01<00:19,  1.87it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍                 | 3/39 [00:01<00:19,  1.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▉                 | 4/39 [00:02<00:18,  1.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▍                | 5/39 [00:02<00:18,  1.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▉                | 6/39 [00:03<00:17,  1.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▍               | 7/39 [00:03<00:17,  1.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▉               | 8/39 [00:04<00:16,  1.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|████▍              | 9/39 [00:04<00:16,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▌             | 10/39 [00:05<00:15,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|█████             | 11/39 [00:05<00:15,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▌            | 12/39 [00:06<00:14,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████            | 13/39 [00:06<00:13,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▍           | 14/39 [00:07<00:13,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▉           | 15/39 [00:08<00:12,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████▍          | 16/39 [00:08<00:12,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▊          | 17/39 [00:09<00:11,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████████▎         | 18/39 [00:09<00:11,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▊         | 19/39 [00:10<00:10,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|█████████▏        | 20/39 [00:10<00:10,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▋        | 21/39 [00:11<00:09,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████▏       | 22/39 [00:11<00:09,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████▌       | 23/39 [00:12<00:08,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|███████████       | 24/39 [00:12<00:08,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|███████████▌      | 25/39 [00:13<00:07,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 26/39 [00:13<00:06,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|████████████▍     | 27/39 [00:14<00:06,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|████████████▉     | 28/39 [00:15<00:05,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▍    | 29/39 [00:15<00:05,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|█████████████▊    | 30/39 [00:16<00:04,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|██████████████▎   | 31/39 [00:16<00:04,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|██████████████▊   | 32/39 [00:17<00:03,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|███████████████▏  | 33/39 [00:17<00:03,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|███████████████▋  | 34/39 [00:18<00:02,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|████████████████▏ | 35/39 [00:18<00:02,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|████████████████▌ | 36/39 [00:19<00:01,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|█████████████████ | 37/39 [00:19<00:01,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|█████████████████▌| 38/39 [00:20<00:00,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████████████| 39/39 [00:20<00:00,  1.88it/s]\u001b[AMetric val_loss improved. New best score: 1.054\n",
      "\n",
      "Epoch 1:  59%|▌| 10/17 [00:01<00:01,  5.05it/s, v_num=9i34, train_loss_step=0.74\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                  | 1/39 [00:00<00:20,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                  | 2/39 [00:01<00:19,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍                 | 3/39 [00:01<00:19,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▉                 | 4/39 [00:02<00:18,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▍                | 5/39 [00:02<00:18,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▉                | 6/39 [00:03<00:17,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▍               | 7/39 [00:03<00:17,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▉               | 8/39 [00:04<00:16,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|████▍              | 9/39 [00:04<00:16,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▌             | 10/39 [00:05<00:15,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|█████             | 11/39 [00:05<00:15,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▌            | 12/39 [00:06<00:14,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████            | 13/39 [00:07<00:14,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▍           | 14/39 [00:07<00:13,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▉           | 15/39 [00:08<00:12,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████▍          | 16/39 [00:08<00:12,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▊          | 17/39 [00:09<00:11,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████████▎         | 18/39 [00:09<00:11,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▊         | 19/39 [00:10<00:10,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|█████████▏        | 20/39 [00:10<00:10,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▋        | 21/39 [00:11<00:09,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████▏       | 22/39 [00:11<00:09,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████▌       | 23/39 [00:12<00:08,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|███████████       | 24/39 [00:13<00:08,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|███████████▌      | 25/39 [00:13<00:07,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 26/39 [00:14<00:07,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|████████████▍     | 27/39 [00:14<00:06,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|████████████▉     | 28/39 [00:15<00:05,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▍    | 29/39 [00:15<00:05,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|█████████████▊    | 30/39 [00:16<00:04,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|██████████████▎   | 31/39 [00:16<00:04,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|██████████████▊   | 32/39 [00:17<00:03,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|███████████████▏  | 33/39 [00:17<00:03,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|███████████████▋  | 34/39 [00:18<00:02,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|████████████████▏ | 35/39 [00:18<00:02,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|████████████████▌ | 36/39 [00:19<00:01,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|█████████████████ | 37/39 [00:20<00:01,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|█████████████████▌| 38/39 [00:20<00:00,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████████████| 39/39 [00:20<00:00,  1.86it/s]\u001b[AMetric val_loss improved by 0.066 >= min_delta = 0.0. New best score: 0.988\n",
      "\n",
      "Epoch 2:  59%|▌| 10/17 [00:02<00:01,  4.98it/s, v_num=9i34, train_loss_step=0.94\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                  | 1/39 [00:00<00:20,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                  | 2/39 [00:01<00:20,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍                 | 3/39 [00:01<00:19,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▉                 | 4/39 [00:02<00:19,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▍                | 5/39 [00:02<00:18,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▉                | 6/39 [00:03<00:17,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▍               | 7/39 [00:03<00:17,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▉               | 8/39 [00:04<00:16,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|████▍              | 9/39 [00:04<00:16,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▌             | 10/39 [00:05<00:15,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|█████             | 11/39 [00:05<00:15,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▌            | 12/39 [00:06<00:14,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████            | 13/39 [00:07<00:14,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▍           | 14/39 [00:07<00:13,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▉           | 15/39 [00:08<00:13,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████▍          | 16/39 [00:08<00:12,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▊          | 17/39 [00:09<00:11,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████████▎         | 18/39 [00:09<00:11,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▊         | 19/39 [00:10<00:10,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|█████████▏        | 20/39 [00:10<00:10,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▋        | 21/39 [00:11<00:09,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████▏       | 22/39 [00:11<00:09,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████▌       | 23/39 [00:12<00:08,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|███████████       | 24/39 [00:13<00:08,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|███████████▌      | 25/39 [00:13<00:07,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 26/39 [00:14<00:07,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|████████████▍     | 27/39 [00:14<00:06,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|████████████▉     | 28/39 [00:15<00:05,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▍    | 29/39 [00:15<00:05,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|█████████████▊    | 30/39 [00:16<00:04,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|██████████████▎   | 31/39 [00:16<00:04,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|██████████████▊   | 32/39 [00:17<00:03,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|███████████████▏  | 33/39 [00:17<00:03,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|███████████████▋  | 34/39 [00:18<00:02,  1.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|████████████████▏ | 35/39 [00:19<00:02,  1.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|████████████████▌ | 36/39 [00:19<00:01,  1.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|█████████████████ | 37/39 [00:20<00:01,  1.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|█████████████████▌| 38/39 [00:20<00:00,  1.83it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████████████| 39/39 [00:21<00:00,  1.86it/s]\u001b[A\n",
      "Epoch 3:  59%|▌| 10/17 [00:01<00:01,  5.01it/s, v_num=9i34, train_loss_step=0.51\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                  | 1/39 [00:00<00:20,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                  | 2/39 [00:01<00:20,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍                 | 3/39 [00:01<00:19,  1.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▉                 | 4/39 [00:02<00:19,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▍                | 5/39 [00:02<00:18,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▉                | 6/39 [00:03<00:18,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▍               | 7/39 [00:03<00:17,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▉               | 8/39 [00:04<00:17,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|████▍              | 9/39 [00:04<00:16,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▌             | 10/39 [00:05<00:15,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|█████             | 11/39 [00:06<00:15,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▌            | 12/39 [00:06<00:14,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████            | 13/39 [00:07<00:14,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▍           | 14/39 [00:07<00:13,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▉           | 15/39 [00:08<00:13,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████▍          | 16/39 [00:08<00:12,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▊          | 17/39 [00:09<00:12,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████████▎         | 18/39 [00:09<00:11,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▊         | 19/39 [00:10<00:10,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|█████████▏        | 20/39 [00:10<00:10,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▋        | 21/39 [00:11<00:09,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████▏       | 22/39 [00:12<00:09,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████▌       | 23/39 [00:12<00:08,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|███████████       | 24/39 [00:13<00:08,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|███████████▌      | 25/39 [00:13<00:07,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 26/39 [00:14<00:07,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|████████████▍     | 27/39 [00:14<00:06,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|████████████▉     | 28/39 [00:15<00:06,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▍    | 29/39 [00:15<00:05,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|█████████████▊    | 30/39 [00:16<00:04,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|██████████████▎   | 31/39 [00:17<00:04,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|██████████████▊   | 32/39 [00:17<00:03,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|███████████████▏  | 33/39 [00:18<00:03,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|███████████████▋  | 34/39 [00:18<00:02,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|████████████████▏ | 35/39 [00:19<00:02,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|████████████████▌ | 36/39 [00:19<00:01,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|█████████████████ | 37/39 [00:20<00:01,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|█████████████████▌| 38/39 [00:20<00:00,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████████████| 39/39 [00:21<00:00,  1.84it/s]\u001b[A\n",
      "Epoch 4:  59%|▌| 10/17 [00:02<00:01,  4.91it/s, v_num=9i34, train_loss_step=0.56\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▍                  | 1/39 [00:00<00:20,  1.83it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|▉                  | 2/39 [00:01<00:20,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|█▍                 | 3/39 [00:01<00:19,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|█▉                 | 4/39 [00:02<00:19,  1.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▍                | 5/39 [00:02<00:18,  1.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▉                | 6/39 [00:03<00:18,  1.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|███▍               | 7/39 [00:03<00:17,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|███▉               | 8/39 [00:04<00:17,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|████▍              | 9/39 [00:04<00:16,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▌             | 10/39 [00:05<00:16,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|█████             | 11/39 [00:06<00:15,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▌            | 12/39 [00:06<00:14,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████            | 13/39 [00:07<00:14,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▍           | 14/39 [00:07<00:13,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|██████▉           | 15/39 [00:08<00:13,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████▍          | 16/39 [00:08<00:12,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|███████▊          | 17/39 [00:09<00:12,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████████▎         | 18/39 [00:09<00:11,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|████████▊         | 19/39 [00:10<00:11,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|█████████▏        | 20/39 [00:11<00:10,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▋        | 21/39 [00:11<00:09,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████▏       | 22/39 [00:12<00:09,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████▌       | 23/39 [00:12<00:08,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|███████████       | 24/39 [00:13<00:08,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|███████████▌      | 25/39 [00:13<00:07,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 26/39 [00:14<00:07,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|████████████▍     | 27/39 [00:14<00:06,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|████████████▉     | 28/39 [00:15<00:06,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▍    | 29/39 [00:16<00:05,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|█████████████▊    | 30/39 [00:16<00:04,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|██████████████▎   | 31/39 [00:17<00:04,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|██████████████▊   | 32/39 [00:17<00:03,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|███████████████▏  | 33/39 [00:18<00:03,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|███████████████▋  | 34/39 [00:18<00:02,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|████████████████▏ | 35/39 [00:19<00:02,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|████████████████▌ | 36/39 [00:19<00:01,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|█████████████████ | 37/39 [00:20<00:01,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|█████████████████▌| 38/39 [00:20<00:00,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████████████| 39/39 [00:21<00:00,  1.83it/s]\u001b[AMonitored metric val_loss did not improve in the last 3 records. Best score: 0.988. Signaling Trainer to stop.\n",
      "\n",
      "Epoch 4:  59%|▌| 10/17 [00:23<00:16,  0.42it/s, v_num=9i34, train_loss_step=0.56\u001b[A\n",
      "RobertaAdapterModel(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttentionWithAdapters(\n",
      "              (query): LoRALinearTorch(\n",
      "                in_features=1024, out_features=1024, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (key): LoRALinearTorch(\n",
      "                in_features=1024, out_features=1024, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (value): LoRALinearTorch(\n",
      "                in_features=1024, out_features=1024, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningLayer(\n",
      "                (prefix_gates): ModuleDict()\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): RobertaSelfOutputWithAdapters(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): LoRALinearTorch(\n",
      "              in_features=1024, out_features=4096, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutputWithAdapters(\n",
      "            (dense): LoRALinearTorch(\n",
      "              in_features=4096, out_features=1024, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (task_adapter): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=1024, out_features=64, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=64, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "          (reft_layer): ReftLayer(\n",
      "            (refts): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (shared_parameters): ModuleDict()\n",
      "    (prefix_tuning): PrefixTuningPool(\n",
      "      (prefix_tunings): ModuleDict()\n",
      "    )\n",
      "    (prompt_tuning): PromptTuningLayer(\n",
      "      (base_model_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
      "      (prompt_tunings): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (default): BertStyleMaskedLMHead(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): Activation_Function_Class(\n",
      "        (f): GELUActivation()\n",
      "      )\n",
      "      (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=1024, out_features=50265, bias=True)\n",
      "    )\n",
      "    (task_adapter): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (2): Activation_Function_Class(\n",
      "        (f): Tanh()\n",
      "      )\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "/user_data/envs/adapter/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:208: Attribute 'adapter_model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['adapter_model'])`.\n",
      "focal loss, alpha\n",
      "[0.536231884057971, 0.605072463768116, 0.5615942028985508, 0.5471014492753623]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.014 MB of 0.014 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch ▁▁▃▃▅▅▅▆▆██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_loss_epoch █▅▄▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss_step ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step ▁▂▃▃▅▅▅▆▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_accuracy ▆█▁▄▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_f1 ▆█▁▄▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss █▁▆▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_precision ▆█▁▄▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_recall ▆█▁▄▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_loss_epoch 0.62128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss_step 0.54944\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 77\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_accuracy 0.6175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_f1 0.6175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 1.02328\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_precision 0.6175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_recall 0.6175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mroberta-Adapter-CE,len=256_ICK-L3_random\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/swguo/PersuTech-adapter-4c-DIL/runs/bgzd9i34\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/swguo/PersuTech-adapter-4c-DIL\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240919_173450-bgzd9i34/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} ../script/train_multiclass.py --project {project} \\\n",
    "--model_name_or_path {model_name_or_path} \\\n",
    "--load_checkpoint {load_checkpoint} \\\n",
    "--task_name {task_name} \\\n",
    "--version {version} \\\n",
    "--strategy 'random' \\\n",
    "--output_dir 'models' \\\n",
    "--train_dataset '../data/NLP4IF2019/Encoder-based-hierarchical/train/Increasing(ICK)/Level' \\\n",
    "--test_dataset '../data/NLP4IF2019/Encoder-based-hierarchical/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user_data/envs/adapter/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-2022-154m and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The 'config' and 'model_name' arguments are specific to the now unsupported legacy Hub repo and will be removed.Please switch to only providing the HF Model Hub identifier.\n",
      "RobertaAdapterModel(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttentionWithAdapters(\n",
      "              (query): LoRALinearTorch(\n",
      "                in_features=1024, out_features=1024, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (key): LoRALinearTorch(\n",
      "                in_features=1024, out_features=1024, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (value): LoRALinearTorch(\n",
      "                in_features=1024, out_features=1024, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningLayer(\n",
      "                (prefix_gates): ModuleDict()\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): RobertaSelfOutputWithAdapters(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): LoRALinearTorch(\n",
      "              in_features=1024, out_features=4096, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutputWithAdapters(\n",
      "            (dense): LoRALinearTorch(\n",
      "              in_features=4096, out_features=1024, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (task_adapter): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=1024, out_features=64, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=64, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "          (reft_layer): ReftLayer(\n",
      "            (refts): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (shared_parameters): ModuleDict()\n",
      "    (prefix_tuning): PrefixTuningPool(\n",
      "      (prefix_tunings): ModuleDict()\n",
      "    )\n",
      "    (prompt_tuning): PromptTuningLayer(\n",
      "      (base_model_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
      "      (prompt_tunings): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (default): BertStyleMaskedLMHead(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): Activation_Function_Class(\n",
      "        (f): GELUActivation()\n",
      "      )\n",
      "      (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=1024, out_features=50265, bias=True)\n",
      "    )\n",
      "    (task_adapter): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (2): Activation_Function_Class(\n",
      "        (f): Tanh()\n",
      "      )\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Dataset Information\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['id', 'text', 'translated_article', 'category', 'None', 'Ethos', 'Logos', 'Pathos', 'classes'],\n",
      "        num_rows: 1234\n",
      "    })\n",
      "})\n",
      "100%|█████████████████████████████████████████| 309/309 [00:13<00:00, 22.85it/s]\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} ../script/inference.py --project {project} \\\n",
    "--base_model_name {model_name_or_path} \\\n",
    "--task_name {task_name} \\\n",
    "--strategy 'random' \\\n",
    "--version 'ICK-L3' \\\n",
    "--best_checkpoint 'best-checkpoint.ckpt' \\\n",
    "--dataset_path_or_name 'NLP4IF2019/Encoder-based-hierarchical/test/dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Eval phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 Accuracy: 0.6829\n",
      "Class 1 Accuracy: 0.5976\n",
      "Class 2 Accuracy: 0.4266\n",
      "Class 3 Accuracy: 0.7241\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} ../script/evaluate_multiclass.py --project {project} \\\n",
    "--task_name {task_name} \\\n",
    "--version 'ICK-L3' \\\n",
    "--strategy 'random' \\\n",
    "--base_model_name {model_name_or_path} \\\n",
    "--dataset_path_or_name 'prediction_ICK-L3.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyberbullying",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
