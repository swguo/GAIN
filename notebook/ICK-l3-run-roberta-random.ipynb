{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persuasive Technical News Detection Task\n",
    "\n",
    "å°ˆæ¡ˆæè¿° \\\n",
    "é€™å€‹å°ˆæ¡ˆæ¡ç”¨ PyTorch Lightning æ¡†æ¶ï¼ŒåŸ·è¡Œ NLP4IF2019 News åˆ†é¡ä»»å‹™ã€‚è¨“ç·´å’Œé©—è­‰éç¨‹æœƒä¸Šå‚³è‡³ Weights & Biases (wandb) å¹³å°ï¼Œä¸¦åœ¨é©—è­‰éšæ®µè¨ˆç®—æ··æ·†çŸ©é™£ï¼ˆconfusion matrixï¼‰å’Œ AUCï¼ˆArea Under the Curveï¼‰åˆ†æ•¸ã€‚\n",
    "\n",
    "NLP4IF2019 News è³‡æ–™é›†ä»‹ç´¹ \\\n",
    "NLP4IF2019 News è³‡æ–™é›†æ˜¯ç”±NLP4IF2019 share taskæ”¶é›†ä¸¦æ•´ç†çš„ä¸€å€‹æ–°èæ•¸æ“šé›†ï¼Œæ•¸æ“šé›†åŒ…æ‹¬350ç¯‡è¨“ç·´æ–‡ç« ã€61ç¯‡é–‹ç™¼æ–‡ç« å’Œ86ç¯‡æ¸¬è©¦æ–‡ç« ï¼Œå…±æœ‰1,2000 Passagesã€‚é€™äº›æ–‡ç« å‡ä¾†è‡ª48å®¶ä¸åŒçš„æ–°èåª’é«”ã€‚æ–‡ç« ä¸­çš„ç‰‡æ®µè¢«æ¨™è¨»äº†18ç¨®å®£å‚³æŠ€è¡“ä¹‹ä¸€ã€‚\n",
    "\n",
    "è³‡æ–™é›†ç‰¹é»ï¼š \\\n",
    "æ•¸æ“šé‡ï¼š 12000 æ¢æ–°èè©•è«– \\\n",
    "æ¨™ç±¤ï¼šå¤šæ¨™ç±¤åˆ†é¡ï¼ˆmulti-label classificationï¼‰ï¼Œå³18ç¨®å®£å‚³æŠ€è¡“ \\\n",
    "èªè¨€ï¼šè‹±æ–‡ï¼ˆEnglishï¼‰ \\\n",
    "æ‡‰ç”¨å ´æ™¯ï¼šæ–‡æœ¬åˆ†é¡ï¼ˆtext classificationï¼‰\n",
    "\n",
    "github : https://github.com/Tariq60/propaganda-nlp4if2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'PersuTech-adapter-4c-DIL'\n",
    "model_name_or_path = 'cardiffnlp/twitter-roberta-large-2022-154m' #'FacebookAI/roberta-large'\n",
    "task_name = \"roberta-Adapter-CE,len=256\"\n",
    "version = 'ICK-L3'\n",
    "load_checkpoint = 'True'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user_data/envs/adapter/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-2022-154m and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "load dataset: /user_data/workspace/CL-PersuTech-4c-DIL/1_adapter/../data/NLP4IF2019/Encoder-based-hierarchical/train/Increasing(ICK)/Level/dataset\n",
      "[0.536231884057971, 0.14492753623188406, 0.18840579710144928, 0.2028985507246377]\n",
      "[0.536231884057971, 0.605072463768116, 0.5615942028985508, 0.5471014492753623]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-2022-154m and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Conver to Embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69/69 [00:01<00:00, 59.90it/s]\n",
      "The 'config' and 'model_name' arguments are specific to the now unsupported legacy Hub repo and will be removed.Please switch to only providing the HF Model Hub identifier.\n",
      "/user_data/envs/adapter/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:208: Attribute 'adapter_model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['adapter_model'])`.\n",
      "focal loss, alpha\n",
      "[0.536231884057971, 0.605072463768116, 0.5615942028985508, 0.5471014492753623]\n",
      "ContrastiveLearningModel(\n",
      "  (encoder): RobertaAdapterModel(\n",
      "    (roberta): RobertaModel(\n",
      "      (embeddings): RobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 1024)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-23): 24 x RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttentionWithAdapters(\n",
      "                (query): LoRALinearTorch(\n",
      "                  in_features=1024, out_features=1024, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (key): LoRALinearTorch(\n",
      "                  in_features=1024, out_features=1024, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (value): LoRALinearTorch(\n",
      "                  in_features=1024, out_features=1024, bias=True\n",
      "                  (loras): ModuleDict()\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (prefix_tuning): PrefixTuningLayer(\n",
      "                  (prefix_gates): ModuleDict()\n",
      "                  (pool): PrefixTuningPool(\n",
      "                    (prefix_tunings): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (output): RobertaSelfOutputWithAdapters(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapters): ModuleDict()\n",
      "                (adapter_fusion_layer): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): LoRALinearTorch(\n",
      "                in_features=1024, out_features=4096, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): RobertaOutputWithAdapters(\n",
      "              (dense): LoRALinearTorch(\n",
      "                in_features=4096, out_features=1024, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (task_adapter): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=1024, out_features=64, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): ReLU()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=64, out_features=1024, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "            (reft_layer): ReftLayer(\n",
      "              (refts): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): RobertaPooler(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (invertible_adapters): ModuleDict()\n",
      "      (shared_parameters): ModuleDict()\n",
      "      (prefix_tuning): PrefixTuningPool(\n",
      "        (prefix_tunings): ModuleDict()\n",
      "      )\n",
      "      (prompt_tuning): PromptTuningLayer(\n",
      "        (base_model_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
      "        (prompt_tunings): ModuleDict()\n",
      "      )\n",
      "    )\n",
      "    (heads): ModuleDict(\n",
      "      (default): BertStyleMaskedLMHead(\n",
      "        (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (1): Activation_Function_Class(\n",
      "          (f): GELUActivation()\n",
      "        )\n",
      "        (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (3): Linear(in_features=1024, out_features=50265, bias=True)\n",
      "      )\n",
      "      (task_adapter): ClassificationHead(\n",
      "        (0): Dropout(p=0.1, inplace=False)\n",
      "        (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (2): Activation_Function_Class(\n",
      "          (f): Tanh()\n",
      "        )\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=1024, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/user_data/envs/adapter/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:44: Attribute 'adapter_model' removed from hparams because it cannot be pickled. You can suppress this warning by setting `self.save_hyperparameters(ignore=['adapter_model'])`.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mswguo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240919_173450-bgzd9i34\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mroberta-Adapter-CE,len=256_ICK-L3_random\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/swguo/PersuTech-adapter-4c-DIL\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/swguo/PersuTech-adapter-4c-DIL/runs/bgzd9i34\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/user_data/envs/adapter/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name    | Type                | Params | Mode\n",
      "-------------------------------------------------------\n",
      "0 | encoder | RobertaAdapterModel | 360 M  | eval\n",
      "-------------------------------------------------------\n",
      "5.3 M     Trainable params\n",
      "355 M     Non-trainable params\n",
      "360 M     Total params\n",
      "1,442.749 Total estimated model params size (MB)\n",
      "/user_data/envs/adapter/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (17) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "Epoch 0:  59%|â–Œ| 10/17 [00:01<00:01,  6.39it/s, v_num=9i34, train_loss_step=1.51\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|â–                  | 1/39 [00:00<00:20,  1.88it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|â–‰                  | 2/39 [00:01<00:19,  1.87it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|â–ˆâ–                 | 3/39 [00:01<00:19,  1.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|â–ˆâ–‰                 | 4/39 [00:02<00:18,  1.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|â–ˆâ–ˆâ–                | 5/39 [00:02<00:18,  1.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|â–ˆâ–ˆâ–‰                | 6/39 [00:03<00:17,  1.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|â–ˆâ–ˆâ–ˆâ–               | 7/39 [00:03<00:17,  1.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|â–ˆâ–ˆâ–ˆâ–‰               | 8/39 [00:04<00:16,  1.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–              | 9/39 [00:04<00:16,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 10/39 [00:05<00:15,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 11/39 [00:05<00:15,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 12/39 [00:06<00:14,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 13/39 [00:06<00:13,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 14/39 [00:07<00:13,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 15/39 [00:08<00:12,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 16/39 [00:08<00:12,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 17/39 [00:09<00:11,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 18/39 [00:09<00:11,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š         | 19/39 [00:10<00:10,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 20/39 [00:10<00:10,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 21/39 [00:11<00:09,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 22/39 [00:11<00:09,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 23/39 [00:12<00:08,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 24/39 [00:12<00:08,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 25/39 [00:13<00:07,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 26/39 [00:13<00:06,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/39 [00:14<00:06,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/39 [00:15<00:05,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/39 [00:15<00:05,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 30/39 [00:16<00:04,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/39 [00:16<00:04,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 32/39 [00:17<00:03,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 33/39 [00:17<00:03,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 34/39 [00:18<00:02,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 35/39 [00:18<00:02,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 36/39 [00:19<00:01,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 37/39 [00:19<00:01,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 38/39 [00:20<00:00,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:20<00:00,  1.88it/s]\u001b[AMetric val_loss improved. New best score: 1.054\n",
      "\n",
      "Epoch 1:  59%|â–Œ| 10/17 [00:01<00:01,  5.05it/s, v_num=9i34, train_loss_step=0.74\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|â–                  | 1/39 [00:00<00:20,  1.86it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|â–‰                  | 2/39 [00:01<00:19,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|â–ˆâ–                 | 3/39 [00:01<00:19,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|â–ˆâ–‰                 | 4/39 [00:02<00:18,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|â–ˆâ–ˆâ–                | 5/39 [00:02<00:18,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|â–ˆâ–ˆâ–‰                | 6/39 [00:03<00:17,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|â–ˆâ–ˆâ–ˆâ–               | 7/39 [00:03<00:17,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|â–ˆâ–ˆâ–ˆâ–‰               | 8/39 [00:04<00:16,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–              | 9/39 [00:04<00:16,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 10/39 [00:05<00:15,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 11/39 [00:05<00:15,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 12/39 [00:06<00:14,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 13/39 [00:07<00:14,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 14/39 [00:07<00:13,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 15/39 [00:08<00:12,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 16/39 [00:08<00:12,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 17/39 [00:09<00:11,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 18/39 [00:09<00:11,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š         | 19/39 [00:10<00:10,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 20/39 [00:10<00:10,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 21/39 [00:11<00:09,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 22/39 [00:11<00:09,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 23/39 [00:12<00:08,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 24/39 [00:13<00:08,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 25/39 [00:13<00:07,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 26/39 [00:14<00:07,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/39 [00:14<00:06,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/39 [00:15<00:05,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/39 [00:15<00:05,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 30/39 [00:16<00:04,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/39 [00:16<00:04,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 32/39 [00:17<00:03,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 33/39 [00:17<00:03,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 34/39 [00:18<00:02,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 35/39 [00:18<00:02,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 36/39 [00:19<00:01,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 37/39 [00:20<00:01,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 38/39 [00:20<00:00,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:20<00:00,  1.86it/s]\u001b[AMetric val_loss improved by 0.066 >= min_delta = 0.0. New best score: 0.988\n",
      "\n",
      "Epoch 2:  59%|â–Œ| 10/17 [00:02<00:01,  4.98it/s, v_num=9i34, train_loss_step=0.94\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|â–                  | 1/39 [00:00<00:20,  1.85it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|â–‰                  | 2/39 [00:01<00:20,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|â–ˆâ–                 | 3/39 [00:01<00:19,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|â–ˆâ–‰                 | 4/39 [00:02<00:19,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|â–ˆâ–ˆâ–                | 5/39 [00:02<00:18,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|â–ˆâ–ˆâ–‰                | 6/39 [00:03<00:17,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|â–ˆâ–ˆâ–ˆâ–               | 7/39 [00:03<00:17,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|â–ˆâ–ˆâ–ˆâ–‰               | 8/39 [00:04<00:16,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–              | 9/39 [00:04<00:16,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 10/39 [00:05<00:15,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 11/39 [00:05<00:15,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 12/39 [00:06<00:14,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 13/39 [00:07<00:14,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 14/39 [00:07<00:13,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 15/39 [00:08<00:13,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 16/39 [00:08<00:12,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 17/39 [00:09<00:11,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 18/39 [00:09<00:11,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š         | 19/39 [00:10<00:10,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 20/39 [00:10<00:10,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 21/39 [00:11<00:09,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 22/39 [00:11<00:09,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 23/39 [00:12<00:08,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 24/39 [00:13<00:08,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 25/39 [00:13<00:07,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 26/39 [00:14<00:07,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/39 [00:14<00:06,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/39 [00:15<00:05,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/39 [00:15<00:05,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 30/39 [00:16<00:04,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/39 [00:16<00:04,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 32/39 [00:17<00:03,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 33/39 [00:17<00:03,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 34/39 [00:18<00:02,  1.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 35/39 [00:19<00:02,  1.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 36/39 [00:19<00:01,  1.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 37/39 [00:20<00:01,  1.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 38/39 [00:20<00:00,  1.83it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:21<00:00,  1.86it/s]\u001b[A\n",
      "Epoch 3:  59%|â–Œ| 10/17 [00:01<00:01,  5.01it/s, v_num=9i34, train_loss_step=0.51\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|â–                  | 1/39 [00:00<00:20,  1.84it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|â–‰                  | 2/39 [00:01<00:20,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|â–ˆâ–                 | 3/39 [00:01<00:19,  1.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|â–ˆâ–‰                 | 4/39 [00:02<00:19,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|â–ˆâ–ˆâ–                | 5/39 [00:02<00:18,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|â–ˆâ–ˆâ–‰                | 6/39 [00:03<00:18,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|â–ˆâ–ˆâ–ˆâ–               | 7/39 [00:03<00:17,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|â–ˆâ–ˆâ–ˆâ–‰               | 8/39 [00:04<00:17,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–              | 9/39 [00:04<00:16,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 10/39 [00:05<00:15,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 11/39 [00:06<00:15,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 12/39 [00:06<00:14,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 13/39 [00:07<00:14,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 14/39 [00:07<00:13,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 15/39 [00:08<00:13,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 16/39 [00:08<00:12,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 17/39 [00:09<00:12,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 18/39 [00:09<00:11,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š         | 19/39 [00:10<00:10,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 20/39 [00:10<00:10,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 21/39 [00:11<00:09,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 22/39 [00:12<00:09,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 23/39 [00:12<00:08,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 24/39 [00:13<00:08,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 25/39 [00:13<00:07,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 26/39 [00:14<00:07,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/39 [00:14<00:06,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/39 [00:15<00:06,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/39 [00:15<00:05,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 30/39 [00:16<00:04,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/39 [00:17<00:04,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 32/39 [00:17<00:03,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 33/39 [00:18<00:03,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 34/39 [00:18<00:02,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 35/39 [00:19<00:02,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 36/39 [00:19<00:01,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 37/39 [00:20<00:01,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 38/39 [00:20<00:00,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:21<00:00,  1.84it/s]\u001b[A\n",
      "Epoch 4:  59%|â–Œ| 10/17 [00:02<00:01,  4.91it/s, v_num=9i34, train_loss_step=0.56\u001b[A\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|â–                  | 1/39 [00:00<00:20,  1.83it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|â–‰                  | 2/39 [00:01<00:20,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|â–ˆâ–                 | 3/39 [00:01<00:19,  1.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|â–ˆâ–‰                 | 4/39 [00:02<00:19,  1.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|â–ˆâ–ˆâ–                | 5/39 [00:02<00:18,  1.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|â–ˆâ–ˆâ–‰                | 6/39 [00:03<00:18,  1.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|â–ˆâ–ˆâ–ˆâ–               | 7/39 [00:03<00:17,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|â–ˆâ–ˆâ–ˆâ–‰               | 8/39 [00:04<00:17,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–              | 9/39 [00:04<00:16,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 10/39 [00:05<00:16,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 11/39 [00:06<00:15,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 12/39 [00:06<00:14,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 13/39 [00:07<00:14,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 14/39 [00:07<00:13,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 15/39 [00:08<00:13,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 16/39 [00:08<00:12,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 17/39 [00:09<00:12,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 18/39 [00:09<00:11,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š         | 19/39 [00:10<00:11,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 20/39 [00:11<00:10,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 21/39 [00:11<00:09,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 22/39 [00:12<00:09,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 23/39 [00:12<00:08,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 24/39 [00:13<00:08,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 25/39 [00:13<00:07,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 26/39 [00:14<00:07,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/39 [00:14<00:06,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 28/39 [00:15<00:06,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 29/39 [00:16<00:05,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 30/39 [00:16<00:04,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/39 [00:17<00:04,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 32/39 [00:17<00:03,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 33/39 [00:18<00:03,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 34/39 [00:18<00:02,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 35/39 [00:19<00:02,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 36/39 [00:19<00:01,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 37/39 [00:20<00:01,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 38/39 [00:20<00:00,  1.81it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39/39 [00:21<00:00,  1.83it/s]\u001b[AMonitored metric val_loss did not improve in the last 3 records. Best score: 0.988. Signaling Trainer to stop.\n",
      "\n",
      "Epoch 4:  59%|â–Œ| 10/17 [00:23<00:16,  0.42it/s, v_num=9i34, train_loss_step=0.56\u001b[A\n",
      "RobertaAdapterModel(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttentionWithAdapters(\n",
      "              (query): LoRALinearTorch(\n",
      "                in_features=1024, out_features=1024, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (key): LoRALinearTorch(\n",
      "                in_features=1024, out_features=1024, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (value): LoRALinearTorch(\n",
      "                in_features=1024, out_features=1024, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningLayer(\n",
      "                (prefix_gates): ModuleDict()\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): RobertaSelfOutputWithAdapters(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): LoRALinearTorch(\n",
      "              in_features=1024, out_features=4096, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutputWithAdapters(\n",
      "            (dense): LoRALinearTorch(\n",
      "              in_features=4096, out_features=1024, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (task_adapter): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=1024, out_features=64, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=64, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "          (reft_layer): ReftLayer(\n",
      "            (refts): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (shared_parameters): ModuleDict()\n",
      "    (prefix_tuning): PrefixTuningPool(\n",
      "      (prefix_tunings): ModuleDict()\n",
      "    )\n",
      "    (prompt_tuning): PromptTuningLayer(\n",
      "      (base_model_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
      "      (prompt_tunings): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (default): BertStyleMaskedLMHead(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): Activation_Function_Class(\n",
      "        (f): GELUActivation()\n",
      "      )\n",
      "      (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=1024, out_features=50265, bias=True)\n",
      "    )\n",
      "    (task_adapter): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (2): Activation_Function_Class(\n",
      "        (f): Tanh()\n",
      "      )\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "/user_data/envs/adapter/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:208: Attribute 'adapter_model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['adapter_model'])`.\n",
      "focal loss, alpha\n",
      "[0.536231884057971, 0.605072463768116, 0.5615942028985508, 0.5471014492753623]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.014 MB of 0.014 MB uploaded\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch â–â–â–ƒâ–ƒâ–…â–…â–…â–†â–†â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_loss_epoch â–ˆâ–…â–„â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss_step â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step â–â–‚â–ƒâ–ƒâ–…â–…â–…â–†â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_accuracy â–†â–ˆâ–â–„â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_f1 â–†â–ˆâ–â–„â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss â–ˆâ–â–†â–„â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_precision â–†â–ˆâ–â–„â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_recall â–†â–ˆâ–â–„â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_loss_epoch 0.62128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss_step 0.54944\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 77\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        val_accuracy 0.6175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              val_f1 0.6175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 1.02328\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_precision 0.6175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          val_recall 0.6175\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run \u001b[33mroberta-Adapter-CE,len=256_ICK-L3_random\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/swguo/PersuTech-adapter-4c-DIL/runs/bgzd9i34\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/swguo/PersuTech-adapter-4c-DIL\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240919_173450-bgzd9i34/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} ../script/train_multiclass.py --project {project} \\\n",
    "--model_name_or_path {model_name_or_path} \\\n",
    "--load_checkpoint {load_checkpoint} \\\n",
    "--task_name {task_name} \\\n",
    "--version {version} \\\n",
    "--strategy 'random' \\\n",
    "--output_dir 'models' \\\n",
    "--train_dataset '../data/NLP4IF2019/Encoder-based-hierarchical/train/Increasing(ICK)/Level' \\\n",
    "--test_dataset '../data/NLP4IF2019/Encoder-based-hierarchical/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user_data/envs/adapter/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-2022-154m and are newly initialized: ['heads.default.3.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The 'config' and 'model_name' arguments are specific to the now unsupported legacy Hub repo and will be removed.Please switch to only providing the HF Model Hub identifier.\n",
      "RobertaAdapterModel(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttentionWithAdapters(\n",
      "              (query): LoRALinearTorch(\n",
      "                in_features=1024, out_features=1024, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (key): LoRALinearTorch(\n",
      "                in_features=1024, out_features=1024, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (value): LoRALinearTorch(\n",
      "                in_features=1024, out_features=1024, bias=True\n",
      "                (loras): ModuleDict()\n",
      "              )\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningLayer(\n",
      "                (prefix_gates): ModuleDict()\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): RobertaSelfOutputWithAdapters(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): LoRALinearTorch(\n",
      "              in_features=1024, out_features=4096, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutputWithAdapters(\n",
      "            (dense): LoRALinearTorch(\n",
      "              in_features=4096, out_features=1024, bias=True\n",
      "              (loras): ModuleDict()\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (task_adapter): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=1024, out_features=64, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=64, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "          (reft_layer): ReftLayer(\n",
      "            (refts): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (shared_parameters): ModuleDict()\n",
      "    (prefix_tuning): PrefixTuningPool(\n",
      "      (prefix_tunings): ModuleDict()\n",
      "    )\n",
      "    (prompt_tuning): PromptTuningLayer(\n",
      "      (base_model_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
      "      (prompt_tunings): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (default): BertStyleMaskedLMHead(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): Activation_Function_Class(\n",
      "        (f): GELUActivation()\n",
      "      )\n",
      "      (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (3): Linear(in_features=1024, out_features=50265, bias=True)\n",
      "    )\n",
      "    (task_adapter): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (2): Activation_Function_Class(\n",
      "        (f): Tanh()\n",
      "      )\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Dataset Information\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['id', 'text', 'translated_article', 'category', 'None', 'Ethos', 'Logos', 'Pathos', 'classes'],\n",
      "        num_rows: 1234\n",
      "    })\n",
      "})\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 309/309 [00:13<00:00, 22.85it/s]\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} ../script/inference.py --project {project} \\\n",
    "--base_model_name {model_name_or_path} \\\n",
    "--task_name {task_name} \\\n",
    "--strategy 'random' \\\n",
    "--version 'ICK-L3' \\\n",
    "--best_checkpoint 'best-checkpoint.ckpt' \\\n",
    "--dataset_path_or_name 'NLP4IF2019/Encoder-based-hierarchical/test/dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Eval phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 Accuracy: 0.6829\n",
      "Class 1 Accuracy: 0.5976\n",
      "Class 2 Accuracy: 0.4266\n",
      "Class 3 Accuracy: 0.7241\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} ../script/evaluate_multiclass.py --project {project} \\\n",
    "--task_name {task_name} \\\n",
    "--version 'ICK-L3' \\\n",
    "--strategy 'random' \\\n",
    "--base_model_name {model_name_or_path} \\\n",
    "--dataset_path_or_name 'prediction_ICK-L3.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyberbullying",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
