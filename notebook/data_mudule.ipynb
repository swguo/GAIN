{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from unit.config import config\n",
    "from unit.dataset_multiclass import get_dataloaders\n",
    "from unit.model import ContrastiveLearningModel\n",
    "import torch\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2a7a1a0f5e4119aa0d4f6154a88213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Conver to Embeddings:   0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader, val_loader  = get_dataloaders(config.BATCH_SIZE,\n",
    "                                            config.NUM_WORKERS, \n",
    "                                            config.MAX_LENGTH,\n",
    "                                            dataset_path_or_name='imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ |#                                                  | 0 Elapsed Time: 0:00:00\n",
      "- | #                                                | 19 Elapsed Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. pair of contrastive\n",
      "torch.Size([4, 128])\n",
      "tensor([[  101,  1045,  4384,  2012,  2320,  2008,  2023,  3185,  2428,  2347],\n",
      "        [  101,  5064,  1010,  2023,  3185,  9020,  2000,  2022,  1999,  5737],\n",
      "        [  101,  1996,  3819,  4028,  2003, 17910,  2098,  2043,  1037,  2564],\n",
      "        [  101,  2748,  1010,  1045,  2113,  1045,  1005,  1049,  2028,  1997]])\n",
      "torch.Size([4, 128])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "torch.Size([4])\n",
      "tensor([1, 1, 0, 0])\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    print(f'{i+1}. pair of contrastive')\n",
    "    input_ids, mask, label = data\n",
    "    print(input_ids.squeeze().size())\n",
    "    print(input_ids.squeeze()[:,:10])\n",
    "    print(mask.squeeze().size())\n",
    "    print(mask.squeeze()[:,:10])\n",
    "    print(label.squeeze().size())\n",
    "    print(label.squeeze())\n",
    "    print('end')\n",
    "    if i >= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. pair of contrastive\n",
      "torch.Size([4, 128])\n",
      "tensor([[  101,  1045,  2293, 16596,  1011, 10882,  1998,  2572,  5627,  2000,\n",
      "          2404,  2039,  2007,  1037,  2843,  1012, 16596,  1011, 10882,  5691,\n",
      "          1013,  2694,  2024,  2788,  2104, 11263, 25848,  1010,  2104,  1011,\n",
      "         12315,  1998, 28947,  1012,  1045,  2699,  2000,  2066,  2023,  1010,\n",
      "          1045,  2428,  2106,  1010,  2021,  2009,  2003,  2000,  2204,  2694,\n",
      "         16596,  1011, 10882,  2004, 17690,  1019,  2003,  2000,  2732, 10313,\n",
      "          1006,  1996,  2434,  1007,  1012, 10021,  4013,  3367, 20086,  2015,\n",
      "          1010, 10036, 19747,  4520,  1010, 25931,  3064, 22580,  1010,  1039,\n",
      "          2290,  2008,  2987,  1005,  1056,  2674,  1996,  4281,  1010,  1998,\n",
      "         16267,  2028,  1011,  8789,  3494,  3685,  2022,  9462,  2007,  1037,\n",
      "          1005, 16596,  1011, 10882,  1005,  4292,  1012,  1006,  1045,  1005,\n",
      "          1049,  2469,  2045,  2024,  2216,  1997,  2017,  2041,  2045,  2040,\n",
      "          2228, 17690,  1019,  2003,  2204, 16596,  1011,   102],\n",
      "        [  101,  4276,  1996,  4024,  3643,  1997,  1037, 12635,  1010,  2926,\n",
      "          2065,  2017,  2066,  2895,  5691,  1012,  2023,  2028,  2838,  1996,\n",
      "          5156,  2482, 29515,  1010,  9590,  2007,  1996,  2307,  3158,  5477,\n",
      "          4168,  5926,  2806,  1010,  5008,  7465,  2007,  1996,  2871,  5806,\n",
      "          7170, 13305,  1010,  1998,  2130,  9452,  2806,  9767,  1012,  2035,\n",
      "          1997,  2023,  2003, 14036,  1998, 17824,  2135,  8971,  2021,  2045,\n",
      "          2003,  2498,  2008,  2428, 13783,  2017,  2185,  2065,  2017,  1005,\n",
      "          2310,  2464,  2115,  3745,  2077,  1012,  1026,  7987,  1013,  1028,\n",
      "          1026,  7987,  1013,  1028,  1996,  5436,  2003,  2081,  5875,  2011,\n",
      "          1996, 10502,  1997,  1037, 10442,  1010,  2029,  2003, 12266,  2021,\n",
      "          6684, 13769,  1012,  2116,  1997,  1996,  3494,  2024,  4600, 12991,\n",
      "         13874,  2094,  1011,  1011,  1996,  4854,  8244,  1010,  1996, 10215,\n",
      "          6206, 12114,  1010,  1996, 15274, 10558,  1010,   102],\n",
      "        [  101,  2049,  1037,  6135,  2779,  2143,  2007,  1037,  2261,  4100,\n",
      "          1011, 10303,  2895, 10071,  2008,  2191,  1996,  5436,  4025,  1037,\n",
      "          2210,  2488,  1998, 10825,  1996, 13972,  1997,  1996,  4438,  3158,\n",
      "          5477,  3152,  1012,  3033,  1997,  1996,  5436,  2123,  1005,  1056,\n",
      "          2191,  3168,  1998,  4025,  2000,  2022,  2794,  1999,  2000,  2224,\n",
      "          2039,  2051,  1012,  1996,  2203,  5436,  2003,  2008,  1997,  1037,\n",
      "          2200,  3937,  2828,  2008,  2987,  1005,  1056,  2681,  1996, 13972,\n",
      "         16986,  1998,  2151, 21438,  2024,  5793,  2013,  1996,  2927,  1012,\n",
      "          1996,  2203,  3496,  2007,  1996, 13109, 19895, 10457,  2123,  1005,\n",
      "          1056,  2191,  3168,  2004,  2027,  2024,  2794,  1999,  1998,  4025,\n",
      "          2000,  2031,  2210, 21923,  2000,  1996,  2381,  1997,  3158,  5477,\n",
      "          1005,  1055,  2839,  1012,  2025,  2428,  4276,  3666,  2153,  1010,\n",
      "          2978,  9364,  1999,  1996,  2203,  2537,  1010,   102],\n",
      "        [  101,  2732,  5790,  1024,  1008,  1008,  1008,  1008,  1008,  5095,\n",
      "          2305,  1008,  1008,  1008,  1008,  5958,  2305,  1008,  1008,  1008,\n",
      "          5958,  2851,  1008,  1008,  4465,  2305,  1008,  6928,  2851,  1026,\n",
      "          7987,  1013,  1028,  1026,  7987,  1013,  1028,  2280,  2047,  5979,\n",
      "         18268,  8872,  2990,  6487,  5178, 13754,  1006,  3744,  8149,  3158,\n",
      "          5477,  4168,  1007,  2003,  2128,  1011,  4137,  2000,  8912,  1010,\n",
      "          1037,  2235,  2021,  6355,  2237,  1999,  3290,  2000,  2393,  1996,\n",
      "          2610,  2045,  2007,  2037,  4073,  2000,  2644,  1037,  2350, 19690,\n",
      "         19535,  3169,  2046,  2037,  2237,  1012,  1996, 12731, 14277, 14778,\n",
      "          2015,  2735,  2041,  2000,  2022,  4654,  1011,  2510,  1010,  2599,\n",
      "          2011,  2280,  3474,  6425, 11527,  2015,  1006,  4459,  2935,  1010,\n",
      "          4728,  2124,  2004, 18626,  2013,  2264,  2203,  2545,  1007,  2040,\n",
      "          2003,  2478,  1037,  2569,  4118,  2002,  4342,   102]])\n",
      "torch.Size([4, 128])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "torch.Size([4])\n",
      "tensor([0, 0, 0, 0])\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(val_loader):\n",
    "    print(f'{i+1}. pair of contrastive')\n",
    "    input_ids, mask, label = data\n",
    "    print(input_ids.squeeze().size())\n",
    "    print(input_ids.squeeze()[:10])\n",
    "    print(mask.squeeze().size())\n",
    "    print(mask.squeeze()[:10])\n",
    "    print(label.squeeze().size())\n",
    "    print(label.squeeze())\n",
    "    print('end')\n",
    "    if i >= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/ |#                                                  | 0 Elapsed Time: 0:00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4c1864ca8b4b3eafe08c83306f4b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ |#                                                  | 0 Elapsed Time: 0:00:00\n",
      "- | #                                                | 20 Elapsed Time: 0:00:00\n",
      "\\ |  #                                               | 25 Elapsed Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0088, -0.1258],\n",
      "        [-0.1019, -0.1434],\n",
      "        [ 0.1229, -0.2430],\n",
      "        [ 0.0328,  0.0125]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7242, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1608, -0.2034],\n",
      "        [-0.0213,  0.0165],\n",
      "        [ 0.2387,  0.0291],\n",
      "        [ 0.1231, -0.0625]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7084, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-1.0900e-01, -2.0336e-01],\n",
      "        [ 4.8706e-02, -1.7089e-02],\n",
      "        [-5.7105e-05, -2.3141e-01],\n",
      "        [-1.1662e-01,  2.6274e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6612, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1076, -0.1642],\n",
      "        [ 0.0751, -0.0298],\n",
      "        [-0.0769, -0.2263],\n",
      "        [ 0.0582,  0.0602]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6678, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.2262, -0.0583],\n",
      "        [ 0.1271, -0.0419],\n",
      "        [ 0.0663, -0.0918],\n",
      "        [-0.0068, -0.1265]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6758, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0182,  0.0193],\n",
      "        [ 0.1333, -0.3757],\n",
      "        [-0.0989, -0.1172],\n",
      "        [ 0.1562,  0.0504]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1966, -0.0893],\n",
      "        [ 0.2278, -0.0369],\n",
      "        [ 0.1962, -0.0968],\n",
      "        [ 0.0213,  0.0096]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0177, -0.2011],\n",
      "        [ 0.0255,  0.0566],\n",
      "        [ 0.0945, -0.0330],\n",
      "        [-0.0502, -0.3976]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6630, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.2638, -0.2398],\n",
      "        [ 0.0670,  0.2459],\n",
      "        [ 0.0113, -0.1016],\n",
      "        [ 0.1644, -0.1516]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0132, -0.1418],\n",
      "        [ 0.0440, -0.0399],\n",
      "        [ 0.0843, -0.1411],\n",
      "        [ 0.0101, -0.2855]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6631, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0835,  0.0269],\n",
      "        [-0.0572, -0.1058],\n",
      "        [-0.2217, -0.4529],\n",
      "        [ 0.2009, -0.2434]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6298, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| |   #                                              | 31 Elapsed Time: 0:00:00\n",
      "/ |    #                                             | 38 Elapsed Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0373, -0.3312],\n",
      "        [-0.0133, -0.1254],\n",
      "        [-0.0854, -0.2331],\n",
      "        [ 0.1365, -0.1071]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0243,  0.0094],\n",
      "        [-0.0765, -0.1042],\n",
      "        [ 0.1357, -0.0343],\n",
      "        [ 0.0178, -0.2212]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7478, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1462, -0.1478],\n",
      "        [-0.1179, -0.4021],\n",
      "        [-0.1580,  0.0294],\n",
      "        [ 0.0380, -0.2000]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7672, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.2283, -0.3274],\n",
      "        [-0.1123,  0.0507],\n",
      "        [-0.0394, -0.0296],\n",
      "        [-0.1492, -0.3911]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7254, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.2042, -0.1308],\n",
      "        [ 0.0480, -0.0906],\n",
      "        [ 0.1976,  0.1979],\n",
      "        [ 0.1704, -0.2233]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7121, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.1552, -0.2187],\n",
      "        [ 0.0953,  0.0059],\n",
      "        [ 0.1867,  0.1283],\n",
      "        [-0.0052, -0.2869]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6727, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.2600,  0.0358],\n",
      "        [ 0.3437,  0.0749],\n",
      "        [ 0.1153, -0.0677],\n",
      "        [-0.1228, -0.0618]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6518, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0758, -0.1206],\n",
      "        [ 0.3280,  0.3955],\n",
      "        [ 0.1304, -0.0756],\n",
      "        [ 0.2399, -0.1410]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6230, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0015,  0.1158],\n",
      "        [-0.0643, -0.2577],\n",
      "        [ 0.1591, -0.0312],\n",
      "        [ 0.0051, -0.2017]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7373, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-7.6151e-05, -4.9755e-03],\n",
      "        [ 2.2062e-01, -4.0708e-02],\n",
      "        [ 5.0877e-03, -2.4706e-01],\n",
      "        [-3.6777e-02, -2.9454e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7298, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0004, -0.2169],\n",
      "        [-0.0526, -0.2423],\n",
      "        [ 0.0117, -0.0294],\n",
      "        [-0.0019, -0.3467]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6969, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.2571,  0.2866],\n",
      "        [-0.0578, -0.0894],\n",
      "        [-0.1156, -0.0210],\n",
      "        [ 0.0534, -0.2254]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6732, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1705, -0.0959],\n",
      "        [ 0.2266, -0.2014],\n",
      "        [ 0.1173, -0.0751],\n",
      "        [-0.0322, -0.0641]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6435, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- |     #                                            | 44 Elapsed Time: 0:00:00\n",
      "\\ |      #                                           | 50 Elapsed Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1168, -0.0658],\n",
      "        [ 0.1266, -0.1389],\n",
      "        [ 0.3482,  0.0443],\n",
      "        [-0.0546, -0.1603]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7044, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0105, -0.4426],\n",
      "        [ 0.1179, -0.2017],\n",
      "        [ 0.0097,  0.0085],\n",
      "        [-0.0511, -0.0033]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0576, -0.3014],\n",
      "        [ 0.0087,  0.1437],\n",
      "        [ 0.2208, -0.2452],\n",
      "        [-0.1920, -0.0799]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7211, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.3749, -0.1218],\n",
      "        [ 0.0832, -0.4684],\n",
      "        [ 0.0586, -0.2173],\n",
      "        [-0.0207, -0.0666]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.8034, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0073, -0.0009],\n",
      "        [ 0.0131, -0.2512],\n",
      "        [-0.0931, -0.0423],\n",
      "        [-0.0045, -0.2352]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6855, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0147,  0.1120],\n",
      "        [ 0.0951,  0.0985],\n",
      "        [-0.0319, -0.0065],\n",
      "        [ 0.1768, -0.1928]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7533, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0905, -0.0740],\n",
      "        [ 0.1745, -0.1013],\n",
      "        [ 0.0537, -0.0737],\n",
      "        [-0.1981,  0.0574]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6278, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0420, -0.1618],\n",
      "        [-0.0366, -0.1477],\n",
      "        [-0.0707, -0.0067],\n",
      "        [ 0.0608, -0.1751]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0433, -0.2398],\n",
      "        [ 0.1451,  0.1271],\n",
      "        [ 0.0876,  0.0232],\n",
      "        [ 0.1874,  0.0463]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7084, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1055, -0.1187],\n",
      "        [ 0.1806, -0.1635],\n",
      "        [ 0.1747, -0.1582],\n",
      "        [ 0.0789, -0.0051]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6831, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1567, -0.1704],\n",
      "        [ 0.1404, -0.1518],\n",
      "        [-0.1835, -0.3576],\n",
      "        [ 0.0953, -0.3221]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7090, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0876,  0.0232],\n",
      "        [ 0.2604, -0.0438],\n",
      "        [ 0.0419, -0.2422],\n",
      "        [-0.1214, -0.2106]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6995, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| |       #                                          | 56 Elapsed Time: 0:00:00\n",
      "/ |        #                                         | 62 Elapsed Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0827, -0.0990],\n",
      "        [-0.0912, -0.1544],\n",
      "        [-0.0547, -0.3322],\n",
      "        [ 0.0859, -0.1605]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6420, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1055, -0.1960],\n",
      "        [-0.0729, -0.1360],\n",
      "        [ 0.0299, -0.1341],\n",
      "        [ 0.1930, -0.0415]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.2558, -0.0950],\n",
      "        [ 0.1059, -0.0866],\n",
      "        [-0.2117,  0.0097],\n",
      "        [-0.0342, -0.2475]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7700, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1458, -0.1865],\n",
      "        [ 0.0650, -0.2731],\n",
      "        [ 0.3736,  0.2580],\n",
      "        [ 0.0866, -0.1007]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6557, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0550, -0.1744],\n",
      "        [ 0.0187, -0.2163],\n",
      "        [-0.0787, -0.3258],\n",
      "        [-0.0120, -0.0905]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6813, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1214, -0.1988],\n",
      "        [-0.0467, -0.4235],\n",
      "        [ 0.2773,  0.1473],\n",
      "        [ 0.1515, -0.1323]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6684, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0920,  0.0326],\n",
      "        [ 0.1748, -0.1137],\n",
      "        [ 0.1203, -0.0977],\n",
      "        [ 0.1484, -0.1982]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6514, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.1131, -0.0903],\n",
      "        [ 0.0654, -0.1907],\n",
      "        [ 0.1661, -0.0484],\n",
      "        [-0.0481, -0.1428]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7064, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0745, -0.0238],\n",
      "        [ 0.1738, -0.1702],\n",
      "        [-0.0633, -0.0418],\n",
      "        [ 0.0084, -0.2567]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6745, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0572,  0.3045],\n",
      "        [ 0.0792, -0.2275],\n",
      "        [-0.0705,  0.0201],\n",
      "        [ 0.2346, -0.2152]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7584, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0406, -0.0066],\n",
      "        [ 0.2417, -0.2807],\n",
      "        [-0.0729, -0.1360],\n",
      "        [ 0.1170, -0.2343]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7250, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0168, -0.0672],\n",
      "        [-0.0361, -0.3143],\n",
      "        [ 0.2071, -0.1532],\n",
      "        [ 0.1212, -0.0652]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6778, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- |         #                                        | 68 Elapsed Time: 0:00:00\n",
      "\\ |          #                                       | 74 Elapsed Time: 0:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0837, -0.1562],\n",
      "        [-0.0057, -0.4267],\n",
      "        [-0.0415, -0.0977],\n",
      "        [-0.1818, -0.1922]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6263, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.1349, -0.4449],\n",
      "        [-0.1282, -0.1696],\n",
      "        [ 0.0711, -0.1420],\n",
      "        [-0.1130, -0.0286]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6700, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0911, -0.2001],\n",
      "        [ 0.2817, -0.1125],\n",
      "        [-0.1083, -0.0040],\n",
      "        [ 0.0743, -0.1560]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0844, -0.0646],\n",
      "        [-0.0835,  0.0471],\n",
      "        [-0.0948, -0.1408],\n",
      "        [ 0.2483,  0.0143]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6417, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1325, -0.2686],\n",
      "        [-0.0380, -0.2686],\n",
      "        [ 0.0733,  0.0186],\n",
      "        [ 0.4105,  0.0765]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7337, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.1167, -0.1131],\n",
      "        [ 0.2375, -0.4445],\n",
      "        [-0.0732, -0.1558],\n",
      "        [ 0.0917,  0.0708]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6358, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0474,  0.0971],\n",
      "        [-0.1463, -0.2057],\n",
      "        [ 0.0124, -0.0042],\n",
      "        [ 0.0680, -0.0006]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6840, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0537, -0.1001],\n",
      "        [ 0.2116, -0.2675],\n",
      "        [ 0.1358, -0.0809],\n",
      "        [ 0.0037, -0.1813]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1275, -0.1432],\n",
      "        [ 0.0423,  0.1333],\n",
      "        [ 0.2023, -0.2977],\n",
      "        [-0.0142, -0.1687]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6448, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0522, -0.0383],\n",
      "        [-0.0328, -0.2676],\n",
      "        [ 0.0007, -0.0770],\n",
      "        [ 0.2424, -0.1526]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0656,  0.0499],\n",
      "        [ 0.0998, -0.1296],\n",
      "        [ 0.2081, -0.0565],\n",
      "        [-0.0539, -0.0177]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1409, -0.0318],\n",
      "        [ 0.0508, -0.0144],\n",
      "        [ 0.1291, -0.0845],\n",
      "        [ 0.1755, -0.0632]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6706, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| |           #                                      | 80 Elapsed Time: 0:00:01\n",
      "/ |            #                                     | 86 Elapsed Time: 0:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0349, -0.2162],\n",
      "        [-0.0134, -0.4468],\n",
      "        [ 0.0755, -0.2106],\n",
      "        [-0.0930, -0.2501]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0587, -0.3069],\n",
      "        [ 0.0136,  0.0540],\n",
      "        [-0.2122, -0.0664],\n",
      "        [ 0.0205, -0.3442]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7013, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0891,  0.0408],\n",
      "        [ 0.3025,  0.3267],\n",
      "        [ 0.0773, -0.1076],\n",
      "        [ 0.0903, -0.1045]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7399, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1531, -0.2016],\n",
      "        [-0.0421, -0.0897],\n",
      "        [ 0.0287,  0.0147],\n",
      "        [-0.0232, -0.0911]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6572, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0625, -0.3006],\n",
      "        [-0.2388, -0.2032],\n",
      "        [ 0.2239, -0.2592],\n",
      "        [ 0.0178, -0.0701]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6743, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.1172, -0.1577],\n",
      "        [ 0.1375, -0.1497],\n",
      "        [ 0.4654,  0.0054],\n",
      "        [ 0.2140, -0.2436]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6351, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1175, -0.2389],\n",
      "        [-0.0650,  0.0495],\n",
      "        [ 0.1279,  0.0906],\n",
      "        [-0.0038, -0.1429]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1207, -0.1018],\n",
      "        [-0.0111, -0.1576],\n",
      "        [ 0.0977,  0.0490],\n",
      "        [ 0.0537, -0.1741]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6855, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.3183, -0.0905],\n",
      "        [-0.0786, -0.1083],\n",
      "        [ 0.0267, -0.1261],\n",
      "        [ 0.1756,  0.0789]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7230, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.3298, -0.0893],\n",
      "        [ 0.0937,  0.0612],\n",
      "        [ 0.2463,  0.0025],\n",
      "        [ 0.0729, -0.3204]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6821, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0929, -0.2928],\n",
      "        [ 0.0180, -0.3205],\n",
      "        [ 0.0926, -0.0781],\n",
      "        [ 0.0109, -0.0104]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6357, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1634, -0.3713],\n",
      "        [ 0.0746, -0.1494],\n",
      "        [ 0.0329,  0.1215],\n",
      "        [ 0.1459,  0.0420]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7971, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- |             #                                    | 92 Elapsed Time: 0:00:01\n",
      "\\ |              #                                   | 98 Elapsed Time: 0:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0173, -0.0092],\n",
      "        [-0.2318, -0.0232],\n",
      "        [-0.0007,  0.0133],\n",
      "        [-0.0876,  0.0580]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1720,  0.0998],\n",
      "        [ 0.1233, -0.1884],\n",
      "        [-0.0074, -0.1082],\n",
      "        [-0.0407,  0.0730]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1773, -0.3191],\n",
      "        [ 0.1117, -0.2301],\n",
      "        [-0.0172, -0.3117],\n",
      "        [ 0.1065, -0.1808]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7417, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0024, -0.0484],\n",
      "        [-0.0608, -0.2856],\n",
      "        [ 0.0316,  0.1195],\n",
      "        [-0.1319, -0.0668]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6422, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0253, -0.1082],\n",
      "        [ 0.0269,  0.0458],\n",
      "        [ 0.1035,  0.0984],\n",
      "        [-0.0440, -0.3460]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7266, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.1407, -0.0939],\n",
      "        [ 0.0825,  0.1203],\n",
      "        [-0.2099, -0.1460],\n",
      "        [-0.0812, -0.0941]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6976, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0696,  0.0947],\n",
      "        [ 0.1992, -0.1945],\n",
      "        [ 0.0179,  0.0096],\n",
      "        [ 0.2748, -0.0854]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6855, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.1663, -0.1527],\n",
      "        [ 0.1144, -0.2481],\n",
      "        [-0.1356, -0.1409],\n",
      "        [-0.0162, -0.1220]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6679, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0759, -0.0630],\n",
      "        [-0.0560, -0.2841],\n",
      "        [ 0.2101, -0.0802],\n",
      "        [-0.0845, -0.1357]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7133, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.1922, -0.1626],\n",
      "        [-0.0414, -0.1857],\n",
      "        [-0.0962, -0.1262],\n",
      "        [-0.0220, -0.2704]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7162, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0365,  0.1625],\n",
      "        [ 0.0788, -0.2211],\n",
      "        [-0.2418, -0.1588],\n",
      "        [ 0.0481, -0.2987]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0302, -0.1776],\n",
      "        [-0.0076, -0.3007],\n",
      "        [ 0.1399, -0.0424],\n",
      "        [-0.0556, -0.5129]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6873, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| |               #                                 | 104 Elapsed Time: 0:00:01\n",
      "/ |                #                                | 110 Elapsed Time: 0:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2873, -0.3260],\n",
      "        [ 0.1250,  0.3054],\n",
      "        [ 0.1980, -0.0582],\n",
      "        [-0.0090, -0.0717]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1280, -0.3536],\n",
      "        [-0.0191, -0.3440],\n",
      "        [ 0.0783, -0.1290],\n",
      "        [ 0.0290, -0.1983]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7530, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1713, -0.0174],\n",
      "        [ 0.1586, -0.0170],\n",
      "        [-0.0544, -0.1224],\n",
      "        [-0.0906,  0.1404]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.1118,  0.0630],\n",
      "        [ 0.0612, -0.2521],\n",
      "        [ 0.0136,  0.0224],\n",
      "        [ 0.3605,  0.0401]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6787, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0005, -0.1304],\n",
      "        [ 0.3042,  0.0089],\n",
      "        [ 0.1504,  0.1410],\n",
      "        [ 0.1255, -0.2242]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7085, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0835, -0.2349],\n",
      "        [-0.0147, -0.0835],\n",
      "        [-0.0633, -0.1725],\n",
      "        [ 0.1868, -0.0110]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6880, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1229, -0.0885],\n",
      "        [-0.0996, -0.2126],\n",
      "        [ 0.0243, -0.4092],\n",
      "        [ 0.1307, -0.0222]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7343, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0035, -0.1062],\n",
      "        [ 0.0763, -0.3428],\n",
      "        [-0.1233, -0.1757],\n",
      "        [ 0.1829, -0.0937]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6765, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0013,  0.0013],\n",
      "        [ 0.1520,  0.0061],\n",
      "        [-0.2351, -0.2733],\n",
      "        [ 0.0514, -0.0839]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.1091,  0.0457],\n",
      "        [ 0.1832, -0.2456],\n",
      "        [ 0.1055, -0.1040],\n",
      "        [ 0.0167, -0.0486]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.1228,  0.0272],\n",
      "        [ 0.1488, -0.2404],\n",
      "        [ 0.0171, -0.2641],\n",
      "        [-0.0250, -0.1682]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.2145,  0.0293],\n",
      "        [-0.1693, -0.1268],\n",
      "        [ 0.0162, -0.1682],\n",
      "        [ 0.0382,  0.0645]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6934, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- |                 #                               | 116 Elapsed Time: 0:00:01\n",
      "\\ |                  #                              | 122 Elapsed Time: 0:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1296, -0.1564],\n",
      "        [ 0.0840,  0.0471],\n",
      "        [-0.1275,  0.1405],\n",
      "        [-0.0492, -0.2031]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6740, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0268, -0.2120],\n",
      "        [ 0.2346, -0.0407],\n",
      "        [ 0.0115, -0.1720],\n",
      "        [ 0.1952, -0.0568]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7101, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1477, -0.4337],\n",
      "        [ 0.1608,  0.1077],\n",
      "        [ 0.0032, -0.3190],\n",
      "        [ 0.1596,  0.0734]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7354, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0273, -0.1475],\n",
      "        [-0.0506, -0.0537],\n",
      "        [ 0.0943, -0.0441],\n",
      "        [ 0.1401, -0.1484]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7284, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0963, -0.0906],\n",
      "        [-0.0260, -0.3970],\n",
      "        [ 0.1298,  0.0529],\n",
      "        [ 0.0782, -0.1084]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6630, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0108, -0.4030],\n",
      "        [ 0.0055, -0.2831],\n",
      "        [ 0.0024, -0.2317],\n",
      "        [ 0.1740, -0.1141]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6850, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0324, -0.1891],\n",
      "        [ 0.0020, -0.4361],\n",
      "        [ 0.0206, -0.0238],\n",
      "        [ 0.0070, -0.1686]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0321, -0.2721],\n",
      "        [-0.0595, -0.2542],\n",
      "        [ 0.0800,  0.0907],\n",
      "        [ 0.0745, -0.0400]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6482, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0541, -0.2611],\n",
      "        [ 0.0272, -0.5046],\n",
      "        [ 0.0461, -0.2737],\n",
      "        [ 0.0206, -0.2389]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0416,  0.2179],\n",
      "        [ 0.1478, -0.2214],\n",
      "        [ 0.0775, -0.2212],\n",
      "        [ 0.0652, -0.1956]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6482, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1168, -0.1112],\n",
      "        [ 0.0246, -0.3120],\n",
      "        [-0.1114, -0.2620],\n",
      "        [ 0.1278, -0.1455]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0018, -0.1057],\n",
      "        [ 0.2993, -0.1489],\n",
      "        [ 0.2347, -0.2850],\n",
      "        [-0.0072, -0.2828]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6800, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| |                   #                             | 128 Elapsed Time: 0:00:01\n",
      "/ |                    #                            | 134 Elapsed Time: 0:00:02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0427,  0.0561],\n",
      "        [-0.2602, -0.1046],\n",
      "        [ 0.1040, -0.1286],\n",
      "        [ 0.1274, -0.0330]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7666, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1770, -0.2424],\n",
      "        [ 0.1553, -0.1453],\n",
      "        [ 0.0956, -0.0019],\n",
      "        [ 0.1019, -0.1293]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6545, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0490, -0.1579],\n",
      "        [ 0.1193, -0.2110],\n",
      "        [ 0.1300, -0.0043],\n",
      "        [ 0.0241, -0.0359]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7282, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1173, -0.2904],\n",
      "        [ 0.1094, -0.0482],\n",
      "        [ 0.1696, -0.1320],\n",
      "        [ 0.3758,  0.2568]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7204, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0047, -0.1697],\n",
      "        [-0.0964, -0.2782],\n",
      "        [-0.0109, -0.0977],\n",
      "        [-0.0370, -0.4194]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7140, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1707,  0.0222],\n",
      "        [-0.0054,  0.0519],\n",
      "        [-0.0158, -0.0829],\n",
      "        [-0.0393, -0.4249]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0307, -0.3674],\n",
      "        [ 0.3115, -0.0981],\n",
      "        [ 0.0776, -0.1306],\n",
      "        [ 0.1307,  0.0258]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7668, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0632, -0.2066],\n",
      "        [ 0.0355, -0.1345],\n",
      "        [-0.0564, -0.0127],\n",
      "        [ 0.1233, -0.1178]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6821, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0791, -0.0170],\n",
      "        [ 0.0557, -0.3006],\n",
      "        [ 0.0779, -0.1283],\n",
      "        [ 0.2370, -0.1053]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6704, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0709, -0.2185],\n",
      "        [ 0.0460, -0.0983],\n",
      "        [ 0.1761,  0.0163],\n",
      "        [-0.0463, -0.0361]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6617, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1076, -0.0815],\n",
      "        [-0.0345,  0.0071],\n",
      "        [-0.2211, -0.0965],\n",
      "        [ 0.3459,  0.0930]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6992, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0329, -0.1090],\n",
      "        [ 0.0045, -0.2810],\n",
      "        [ 0.0132, -0.0983],\n",
      "        [-0.1521, -0.1372]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7294, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- |                     #                           | 140 Elapsed Time: 0:00:02\n",
      "\\ |                      #                          | 146 Elapsed Time: 0:00:02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0164, -0.2176],\n",
      "        [-0.2525, -0.1048],\n",
      "        [ 0.1174, -0.0762],\n",
      "        [ 0.1088, -0.0261]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7312, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0991, -0.1140],\n",
      "        [ 0.2457, -0.3058],\n",
      "        [ 0.1596, -0.1159],\n",
      "        [-0.0107, -0.2472]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7396, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0786, -0.1083],\n",
      "        [ 0.0701, -0.0371],\n",
      "        [ 0.1875,  0.0177],\n",
      "        [ 0.1614, -0.2057]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6486, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0100, -0.1140],\n",
      "        [ 0.1759, -0.0131],\n",
      "        [ 0.0487, -0.0171],\n",
      "        [ 0.0255, -0.1649]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0896, -0.3578],\n",
      "        [ 0.1657, -0.2814],\n",
      "        [ 0.1232, -0.0836],\n",
      "        [ 0.0763, -0.1831]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6739, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.2116, -0.1204],\n",
      "        [ 0.1064, -0.1288],\n",
      "        [ 0.2232, -0.1009],\n",
      "        [-0.0353,  0.0915]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0500, -0.1871],\n",
      "        [ 0.0911,  0.1733],\n",
      "        [ 0.0110,  0.0757],\n",
      "        [ 0.0409, -0.1391]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0371, -0.2941],\n",
      "        [-0.0145, -0.2276],\n",
      "        [ 0.0586, -0.2321],\n",
      "        [ 0.0108, -0.2174]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0824,  0.1085],\n",
      "        [ 0.1657, -0.1780],\n",
      "        [-0.0498, -0.1547],\n",
      "        [ 0.0798, -0.1862]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7278, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0120, -0.1795],\n",
      "        [-0.0786, -0.2309],\n",
      "        [-0.0333, -0.1359],\n",
      "        [-0.4458, -0.3460]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7383, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.2067, -0.1328],\n",
      "        [ 0.0814, -0.1679],\n",
      "        [-0.0366, -0.0540],\n",
      "        [ 0.1099, -0.0206]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6441, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0035, -0.3327],\n",
      "        [ 0.0871, -0.0282],\n",
      "        [-0.1051, -0.2180],\n",
      "        [ 0.0452, -0.1881]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7123, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| |                       #                         | 152 Elapsed Time: 0:00:02\n",
      "/ |                        #                        | 158 Elapsed Time: 0:00:02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0217, -0.3026],\n",
      "        [ 0.0483, -0.1300],\n",
      "        [ 0.0296, -0.1013],\n",
      "        [ 0.0803, -0.0682]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6753, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.2560, -0.0914],\n",
      "        [-0.0041,  0.0123],\n",
      "        [ 0.1052, -0.2322],\n",
      "        [-0.0680, -0.0026]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6410, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1283,  0.1828],\n",
      "        [ 0.0273,  0.0747],\n",
      "        [ 0.2288, -0.2117],\n",
      "        [ 0.0017,  0.0709]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6403, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0891, -0.1774],\n",
      "        [ 0.1181, -0.4283],\n",
      "        [-0.0851, -0.0204],\n",
      "        [ 0.1859, -0.0706]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7844, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0098, -0.0497],\n",
      "        [ 0.0144, -0.2797],\n",
      "        [ 0.1092, -0.0320],\n",
      "        [ 0.0892,  0.0173]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6816, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.2060, -0.2181],\n",
      "        [ 0.2867,  0.0536],\n",
      "        [-0.1150, -0.1064],\n",
      "        [ 0.2243, -0.0954]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7469, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.1953, -0.1541],\n",
      "        [ 0.0980, -0.0866],\n",
      "        [ 0.0741, -0.3513],\n",
      "        [-0.1409,  0.0424]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7133, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.1057, -0.3077],\n",
      "        [ 0.0449,  0.0376],\n",
      "        [ 0.0154, -0.0843],\n",
      "        [ 0.0963,  0.2415]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7273, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.2112, -0.3247],\n",
      "        [ 0.0515, -0.0745],\n",
      "        [ 0.0212, -0.2190],\n",
      "        [-0.1718, -0.1200]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6452, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0811,  0.1818],\n",
      "        [ 0.0240, -0.0966],\n",
      "        [ 0.2155, -0.2329],\n",
      "        [ 0.0163, -0.1974]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7819, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0188, -0.3495],\n",
      "        [ 0.0219, -0.1287],\n",
      "        [-0.0133, -0.3592],\n",
      "        [-0.0130, -0.2484]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.1080, -0.2066],\n",
      "        [ 0.1194,  0.0766],\n",
      "        [-0.0296, -0.1167],\n",
      "        [ 0.0778, -0.0887]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6806, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "- |                         #                       | 164 Elapsed Time: 0:00:02\n",
      "\\ |                           #                     | 170 Elapsed Time: 0:00:02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0925, -0.0484],\n",
      "        [ 0.1172, -0.2793],\n",
      "        [ 0.1034, -0.1441],\n",
      "        [-0.0431,  0.0852]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6488, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1941, -0.1178],\n",
      "        [-0.0728, -0.3562],\n",
      "        [-0.1660, -0.3855],\n",
      "        [ 0.1804,  0.0826]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6657, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1066, -0.3028],\n",
      "        [ 0.1320, -0.2094],\n",
      "        [ 0.2466,  0.3388],\n",
      "        [ 0.2192, -0.2880]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6682, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.1423,  0.0083],\n",
      "        [ 0.0425, -0.2610],\n",
      "        [-0.0986, -0.2032],\n",
      "        [ 0.0353, -0.2353]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6716, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.2006,  0.0146],\n",
      "        [ 0.1027, -0.0280],\n",
      "        [ 0.0992,  0.0923],\n",
      "        [ 0.0519,  0.0023]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7274, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0097, -0.3095],\n",
      "        [ 0.0827, -0.0618],\n",
      "        [ 0.1379, -0.1968],\n",
      "        [-0.2102, -0.3630]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0481,  0.0134],\n",
      "        [-0.0921, -0.1592],\n",
      "        [ 0.1083, -0.2772],\n",
      "        [-0.0798,  0.0839]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6838, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0069, -0.2049],\n",
      "        [ 0.1674, -0.0706],\n",
      "        [-0.0953, -0.1793],\n",
      "        [ 0.0251, -0.1362]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7229, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1281, -0.2663],\n",
      "        [ 0.1879, -0.1144],\n",
      "        [ 0.0572, -0.1873],\n",
      "        [-0.1539, -0.1251]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6426, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0225, -0.0093],\n",
      "        [ 0.2060, -0.0587],\n",
      "        [ 0.2131,  0.0033],\n",
      "        [-0.0921, -0.1552]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6998, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1241, -0.1147],\n",
      "        [-0.2095,  0.1107],\n",
      "        [ 0.1527,  0.0122],\n",
      "        [ 0.0527, -0.2455]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.6365, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0048, -0.3872],\n",
      "        [ 0.0184, -0.0124],\n",
      "        [ 0.1314, -0.2781],\n",
      "        [ 0.1033, -0.0451]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7207, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| |                            #                    | 176 Elapsed Time: 0:00:02\n",
      "/ |                             #                   | 182 Elapsed Time: 0:00:02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1170, -0.1331],\n",
      "        [ 0.0305,  0.1134],\n",
      "        [ 0.0951,  0.1754],\n",
      "        [-0.0202, -0.1046]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1087, -0.2300],\n",
      "        [ 0.0353, -0.2499],\n",
      "        [ 0.0278, -0.1014],\n",
      "        [ 0.2850, -0.1396]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7141, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.1285,  0.0460],\n",
      "        [ 0.2358,  0.0134],\n",
      "        [-0.1887, -0.1046],\n",
      "        [ 0.2155, -0.0650]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6840, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0320, -0.1925],\n",
      "        [ 0.0616, -0.0064],\n",
      "        [ 0.0843, -0.1988],\n",
      "        [-0.2794, -0.2928]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.1566, -0.0328],\n",
      "        [-0.2097, -0.0830],\n",
      "        [ 0.1110, -0.2440],\n",
      "        [-0.0230, -0.1346]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.7881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0.0244, -0.5938],\n",
      "        [ 0.0232,  0.1045],\n",
      "        [-0.1174, -0.2099],\n",
      "        [ 0.0720, -0.0090]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([1, 1, 0, 0], device='cuda:0')\n",
      "tensor(0.7510, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[-0.0475, -0.2138],\n",
      "        [-0.1024, -0.1891],\n",
      "        [-0.1045, -0.0972],\n",
      "        [ 0.2239,  0.1395]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([0, 0, 1, 1], device='cuda:0')\n",
      "tensor(0.6725, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3775246/1471503922.py\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/cyberbullying/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/Framework/ContrastiveLearning Lightning Framework/notebook/../unit/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         outputs = self.encoder(input_ids=input_ids.squeeze(), \n\u001b[1;32m     36\u001b[0m                                attention_mask=attention_mask.squeeze())\n",
      "\u001b[0;32m~/envs/cyberbullying/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/cyberbullying/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         )\n\u001b[0;32m--> 990\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/cyberbullying/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/cyberbullying/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    580\u001b[0m                 )\n\u001b[1;32m    581\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    583\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/cyberbullying/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/cyberbullying/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/cyberbullying/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/cyberbullying/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     ):\n\u001b[0;32m--> 401\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/cyberbullying/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/cyberbullying/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Normalize the attention scores to probabilities.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# This is actually dropping out entire tokens to attend to, which might\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/cyberbullying/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/cyberbullying/lib/python3.8/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/cyberbullying/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1843\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1844\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1845\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# 加载最佳模型检查点\n",
    "model = ContrastiveLearningModel(model_name=config.MODEL_NAME)\n",
    "\n",
    "# 进行推理和评估\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "for i, batch in enumerate(tqdm(train_loader)):\n",
    "    input_ids, attention_mask, label = [t.squeeze().to(device) for t in batch]\n",
    "    output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    print(output)\n",
    "    print(label)\n",
    "    loss = torch.nn.functional.cross_entropy(output,label)\n",
    "    print(loss)\n",
    "    '''\n",
    "    pred = torch.argmax(output, dim=1)\n",
    "    val_preds = pred.cpu()\n",
    "    val_labels = label.cpu()\n",
    "    precision = precision_score(val_labels, val_preds, average='micro')\n",
    "    recall = recall_score(val_labels, val_preds, average='micro')\n",
    "    f1 = f1_score(val_labels, val_preds, average='micro')\n",
    "    accuracy = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    print('val_precision', precision)\n",
    "    print('val_recall', recall)\n",
    "    print('val_f1', f1)\n",
    "    print('val_accuracy', accuracy)\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始矩陣形狀: (2, 4, 5)\n",
      "降維後的矩陣形狀: (8, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 建立一個形狀為 2x4x128 的矩陣\n",
    "matrix = np.random.rand(2, 4, 5)\n",
    "\n",
    "# 獲取矩陣的形狀\n",
    "shape = matrix.shape\n",
    "\n",
    "# 計算前兩個維度的乘積\n",
    "new_shape_first_dim = shape[0] * shape[1]\n",
    "\n",
    "# 自動設置新的形狀\n",
    "reshaped_matrix = matrix.reshape(new_shape_first_dim, shape[2])\n",
    "\n",
    "print(\"原始矩陣形狀:\", matrix.shape)\n",
    "print(\"降維後的矩陣形狀:\", reshaped_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.79557872, 0.46039835, 0.56563753, 0.7488221 , 0.46439358],\n",
       "        [0.7928888 , 0.96784006, 0.08237206, 0.02275389, 0.1669453 ],\n",
       "        [0.15931366, 0.47206555, 0.57146352, 0.1049263 , 0.31776827],\n",
       "        [0.36920129, 0.80437327, 0.14996319, 0.16418124, 0.94332088]],\n",
       "\n",
       "       [[0.31249487, 0.88598467, 0.34534662, 0.46421781, 0.0354572 ],\n",
       "        [0.0720262 , 0.80569732, 0.10087847, 0.70427672, 0.43372558],\n",
       "        [0.75418592, 0.34902049, 0.03212869, 0.58542441, 0.61343425],\n",
       "        [0.86287771, 0.46624563, 0.26669618, 0.88591575, 0.13587397]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79557872, 0.46039835, 0.56563753, 0.7488221 , 0.46439358],\n",
       "       [0.7928888 , 0.96784006, 0.08237206, 0.02275389, 0.1669453 ],\n",
       "       [0.15931366, 0.47206555, 0.57146352, 0.1049263 , 0.31776827],\n",
       "       [0.36920129, 0.80437327, 0.14996319, 0.16418124, 0.94332088],\n",
       "       [0.31249487, 0.88598467, 0.34534662, 0.46421781, 0.0354572 ],\n",
       "       [0.0720262 , 0.80569732, 0.10087847, 0.70427672, 0.43372558],\n",
       "       [0.75418592, 0.34902049, 0.03212869, 0.58542441, 0.61343425],\n",
       "       [0.86287771, 0.46624563, 0.26669618, 0.88591575, 0.13587397]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyberbullying",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
